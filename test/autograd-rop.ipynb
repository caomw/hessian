{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function clone (t) -- deep-copy a table\n",
    "    if type(t) ~= \"table\" then return t end\n",
    "    local meta = getmetatable(t)\n",
    "    local target = {}\n",
    "    for k, v in pairs(t) do\n",
    "        if type(v) == \"table\" then\n",
    "            target[k] = clone(v)\n",
    "        else\n",
    "            target[k] = v\n",
    "        end\n",
    "    end\n",
    "    setmetatable(target, meta)\n",
    "    return target\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- some data:\n",
    "x = torch.randn(1,20)\n",
    "y = torch.Tensor(1,10):zero() y[1][3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- R-op (mine) but this is more complicated than Hv = Grad(g^T v) method. \n",
    "-- I'm taking the ddf = grad_r (grad_w(w + rv) ), and takes the r part of ddf\n",
    "-- I should ditch this and implement the easier one. \n",
    "\n",
    "local d = require 'autograd'\n",
    "local t = require 'torch'\n",
    "d.optimize(true)\n",
    "\n",
    "params = {\n",
    "   W = {\n",
    "      t.randn(20,15),\n",
    "      t.randn(15,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   },\n",
    "    r = {\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    },\n",
    "    v = {\n",
    "           -- has to be the same vector as in Hv\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    },\n",
    "    Wb ={    W = {\n",
    "      t.randn(20,15),\n",
    "      t.randn(15,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   }}\n",
    "}\n",
    "-- define model\n",
    "local innerFn = function(params, x, y)\n",
    "   local h1 = t.tanh(x * params.W[1] + params.b[1])\n",
    "   local h2 = t.tanh(h1 * params.W[2] + params.b[2])\n",
    "   local yHat = h2 - t.log(t.sum(t.exp(h2)))\n",
    "   local loss = - t.sum(t.cmul(yHat, y))\n",
    "   return loss\n",
    "end\n",
    "\n",
    "--print(#params2.r.W)\n",
    "\n",
    "--TO DO : change params2.Wb + t.cmul(params2.r, params2.v) so that it handles the nested params nicely. \n",
    "local outerFn = function(params)\n",
    "    local product = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            product.W[i] = t.cmul(params.r.W[i], params.v.W[i]) \n",
    "            product.b[i] = t.cmul(params.r.b[i], params.v.b[i]) \n",
    "    end\n",
    "    local addition = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            addition.W[i] = t.add(params.W[i], product.W[i])\n",
    "            addition.b[i] = t.add(params.b[i], product.b[i])\n",
    "    end   \n",
    "    local grads, loss = d(innerFn)(addition, x, y)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "local ddf = d(outerFn)\n",
    "--gradGrads = d(outerFn)(params)\n",
    "\n",
    "\n",
    "-- local params2 = {\n",
    "--     W = 5,\n",
    "--     r = 3,\n",
    "--     v = 2\n",
    "-- }\n",
    "\n",
    "gradGrads = ddf(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Columns 1 to 6\n",
       "-1.2319e-04  3.4769e-11 -1.9364e-06  4.0355e-06 -1.8190e-03 -1.3384e-09\n",
       "-4.3352e-05  6.5389e-12 -3.9772e-06  2.2154e-05 -1.7886e-03  1.3819e-10\n",
       "-2.3108e-04 -3.0768e-11  6.8264e-07  3.3153e-05 -1.6447e-03  1.2800e-09\n",
       "-1.5329e-03 -2.4090e-11 -1.8689e-05  7.8916e-05  4.2790e-03 -1.7870e-08\n",
       "-1.0816e-03 -1.1619e-10  2.7880e-06  5.4621e-05  3.1012e-03  1.6503e-09\n",
       "-4.2721e-04  1.0758e-10 -1.9012e-06 -3.6210e-05  7.2588e-04  1.4795e-09\n",
       " 8.6955e-04 -2.1163e-10  6.4921e-06  3.9250e-05 -5.6689e-04  3.1359e-10\n",
       "-9.6196e-04  9.6830e-11  1.0248e-05  6.1108e-05 -4.2449e-03 -4.5249e-09\n",
       "-7.2045e-04  5.7632e-11 -1.0357e-05 -3.5908e-05  8.3325e-03  6.2060e-09\n",
       " 7.5839e-04  2.2670e-11 -2.5747e-06 -8.4386e-06 -5.2687e-03 -8.5392e-09\n",
       "-4.8854e-06 -2.9747e-12 -1.8510e-07 -6.0776e-07  7.4283e-05 -2.0005e-10\n",
       "-4.1352e-04 -5.0327e-11 -4.4916e-06  6.7271e-06  9.6907e-04 -5.1175e-10\n",
       " 1.4079e-03 -2.5326e-10  1.4732e-05  5.6994e-06 -2.0705e-02 -3.3537e-09\n",
       "-2.5517e-03  9.7412e-11 -1.8903e-06 -3.5973e-05 -7.8465e-03  1.6774e-08\n",
       "-6.7864e-05 -1.0412e-11  4.5546e-06  5.5291e-06 -3.0148e-03 -2.3515e-09\n",
       "-2.1010e-04 -3.4090e-11  5.8330e-06 -4.7726e-06 -2.2390e-03  1.9961e-09\n",
       "-2.6734e-05  7.7077e-12  1.1205e-06  1.6548e-05 -3.8353e-05  1.2919e-09\n",
       "-2.2347e-04 -1.5697e-11  1.9014e-06  1.8074e-06  2.2132e-04 -1.3719e-09\n",
       "-3.3660e-04 -1.0327e-11 -5.6639e-06  3.2749e-05 -1.2639e-03 -8.1198e-09\n",
       " 4.0958e-04  1.9691e-10 -1.2502e-05 -2.0070e-05 -1.7903e-03 -1.6432e-08\n",
       "\n",
       "Columns 7 to 12\n",
       "-9.1197e-11  3.9549e-04 -1.1527e-05  6.6259e-08 -2.3475e-03 -1.8700e-07\n",
       " 6.4563e-10  8.8089e-04 -1.0974e-05  3.8529e-08  3.1622e-03  6.6386e-07\n",
       " 2.9999e-10 -9.4110e-04  1.0711e-04  1.6883e-07 -2.0256e-03  3.0249e-06\n",
       " 9.5882e-10 -8.5249e-03  6.7743e-04 -8.2025e-07  6.1434e-03 -5.7798e-06\n",
       "-9.9538e-10 -6.8796e-04  3.2655e-05 -5.0787e-07  8.9883e-03 -2.9854e-06\n",
       "-5.5252e-10 -2.1157e-03 -2.4042e-04 -1.8811e-07  1.7571e-03  2.1726e-07\n",
       "-1.5352e-09  3.3772e-04  2.5252e-05  6.6051e-08 -1.2179e-03 -2.0666e-06\n",
       " 3.5980e-10  5.4844e-04  1.1319e-06 -4.5765e-07 -1.1995e-03  3.5108e-06\n",
       " 2.4441e-09 -1.5967e-03 -1.4815e-04  6.9557e-07 -6.2692e-04 -5.6736e-06\n",
       " 1.5277e-09 -1.6556e-03 -4.2154e-05 -1.7950e-07  7.0740e-03 -1.5837e-06\n",
       " 4.6191e-11 -3.6733e-05 -4.1778e-08 -1.4792e-08  2.7966e-05 -8.1372e-08\n",
       "-8.1288e-10 -5.9537e-04  7.9456e-05  1.1210e-07 -5.4369e-04  9.0287e-08\n",
       "-6.0861e-10 -7.0276e-03  2.4437e-04 -1.2792e-06  1.0292e-04 -1.1588e-05\n",
       " 2.6004e-09 -3.8767e-03 -6.2496e-04 -3.8901e-07 -2.4714e-03  3.9879e-06\n",
       "-4.5655e-10  2.2927e-04 -1.0627e-05  2.3612e-08 -2.2685e-03  2.0001e-06\n",
       "-1.3420e-10  1.1505e-04  1.6125e-05 -4.8602e-08  2.7019e-03  2.8376e-06\n",
       " 3.9149e-10  4.7685e-04  7.9019e-06  5.2833e-08 -3.7065e-04 -3.0891e-07\n",
       "-4.9372e-11  1.9670e-04 -1.0127e-04 -9.5497e-08  5.3530e-04 -5.5166e-07\n",
       " 3.0305e-10 -4.2565e-04 -1.1514e-04  8.1342e-08  4.2459e-03  1.3672e-06\n",
       "-1.1179e-09 -5.2465e-03  2.1696e-04 -3.1279e-07 -1.0236e-03 -7.6138e-06\n",
       "\n",
       "Columns 13 to 15\n",
       " 5.8726e-08 -1.8798e-05 -1.7028e-03\n",
       "-7.2776e-08  5.5870e-05 -3.5704e-04\n",
       " 5.8575e-08 -5.5039e-05  2.6542e-03\n",
       "-5.6599e-07  1.0526e-04 -4.6609e-02\n",
       "-3.4057e-07 -2.2971e-04  2.8215e-04\n",
       "-1.1299e-07 -2.5156e-07 -3.7184e-03\n",
       "-1.5085e-07 -6.1525e-05  1.8772e-02\n",
       "-3.7956e-08 -1.6317e-04 -1.5767e-03\n",
       " 5.2506e-07  1.8053e-05  5.3751e-03\n",
       " 1.6097e-08  1.0088e-04 -1.7109e-02\n",
       "-9.4181e-09  5.4656e-07  3.5047e-04\n",
       " 9.8311e-08  5.3667e-05 -4.5365e-04\n",
       " 6.3418e-07  2.8935e-05 -8.2701e-03\n",
       " 2.3850e-07 -1.3460e-04 -3.9253e-03\n",
       " 9.4738e-08  5.3301e-05  5.3120e-03\n",
       " 1.4214e-07 -3.6427e-05 -6.9480e-03\n",
       "-1.1828e-08  1.0206e-05  1.3503e-03\n",
       " 4.4953e-08 -1.3992e-05  1.1111e-03\n",
       "-8.8630e-08  6.1704e-05  1.2061e-03\n",
       "-6.3685e-08 -7.0145e-05  6.7897e-03\n",
       "[torch.DoubleTensor of size 20x15]\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradGrads.r.W[1].raw "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0  1\n",
       "[torch.DoubleTensor of size 1x2]\n",
       "\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local input_size = 2\n",
    "local hidden_size1 = 3\n",
    "local output_size = 2\n",
    "x = torch.randn(1,input_size)\n",
    "y = torch.Tensor(1,output_size):zero() y[1][2] = 1\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "local d = require 'autograd'\n",
    "local t = require 'torch'\n",
    "d.optimize(true)\n",
    "\n",
    "local input_size = 2\n",
    "local hidden_size1 = 3\n",
    "local output_size = 2\n",
    "\n",
    "-- some data:\n",
    "x = torch.randn(1,input_size)\n",
    "y = torch.Tensor(1,output_size):zero() y[1][2] = 1\n",
    "\n",
    "params = {\n",
    "   W = {\n",
    "      t.randn(input_size,hidden_size1),\n",
    "      t.randn(hidden_size1,output_size),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(hidden_size1),\n",
    "      t.randn(output_size),\n",
    "   },\n",
    "    r = {\n",
    "            W = {\n",
    "              t.randn(input_size, hidden_size1),\n",
    "              t.randn(hidden_size1, output_size),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(hidden_size1),\n",
    "              t.randn(output_size),\n",
    "               }\n",
    "    },\n",
    "    v = {\n",
    "           -- has to be the same vector as in Hv\n",
    "            W = {\n",
    "              t.randn(input_size, hidden_size1),\n",
    "              t.randn(hidden_size1, output_size),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(hidden_size1),\n",
    "              t.randn(output_size),\n",
    "               }\n",
    "    },\n",
    "    Wb ={    W = {\n",
    "      t.randn(input_size, hidden_size1),\n",
    "      t.randn(hidden_size1,output_size),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(hidden_size1),\n",
    "      t.randn(output_size),\n",
    "   }}\n",
    "}\n",
    "-- define model\n",
    "local innerFn = function(params, x, y)\n",
    "   local h1 = t.tanh(x * params.W[1] + params.b[1])\n",
    "   local h2 = t.tanh(h1 * params.W[2] + params.b[2])\n",
    "   local yHat = h2 - t.log(t.sum(t.exp(h2)))\n",
    "   local loss = - t.sum(t.cmul(yHat, y))\n",
    "   return loss\n",
    "end\n",
    "\n",
    "--print(#params2.r.W)\n",
    "\n",
    "--TO DO : change params2.Wb + t.cmul(params2.r, params2.v) so that it handles the nested params nicely. \n",
    "local outerFn = function(params)\n",
    "    local product = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            product.W[i] = t.cmul(params.r.W[i], params.v.W[i]) \n",
    "            product.b[i] = t.cmul(params.r.b[i], params.v.b[i]) \n",
    "    end\n",
    "    local addition = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            addition.W[i] = t.add(params.W[i], product.W[i])\n",
    "            addition.b[i] = t.add(params.b[i], product.b[i])\n",
    "    end   \n",
    "    local grads, loss = d(innerFn)(addition, x, y)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "local ddf = d(outerFn)\n",
    "--gradGrads = d(outerFn)(params)\n",
    "\n",
    "\n",
    "-- local params2 = {\n",
    "--     W = 5,\n",
    "--     r = 3,\n",
    "--     v = 2\n",
    "-- }\n",
    "\n",
    "gradGrads = ddf(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
