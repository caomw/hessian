{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- Libs\n",
    "local grad = require 'autograd'\n",
    "local util = require 'autograd.util'\n",
    "local lossFuns = require 'autograd.loss'\n",
    "local optim = require 'optim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.optimize(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- define trainable parameters:\n",
    "params = {\n",
    "   W = {\n",
    "      t.randn(100,50),\n",
    "      t.randn(50,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(50),\n",
    "      t.randn(10),\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- define model\n",
    "neuralNet = function(params, x, y)\n",
    "   local h1 = t.tanh(x * params.W[1] + params.b[1])\n",
    "   local h2 = t.tanh(h1 * params.W[2] + params.b[2])\n",
    "   local yHat = h2 - t.log(t.sum(t.exp(h2)))\n",
    "   local loss = - t.sum(t.cmul(yHat, y))\n",
    "   return loss\n",
    "end "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- get gradients:\n",
    "dneuralNet = grad(neuralNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function: 0x0c9b74d0\t\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dneuralNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function: 0x0c998dc8\t\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(neuralNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "-- some data:\n",
    "x = t.randn(1,100)\n",
    "y = t.Tensor(1,10):zero() y[1][3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- compute loss and gradients wrt all parameters in params:\n",
    "dparams, loss = dneuralNet(params, x, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  b : \n",
       "    {\n",
       "      1 : \n",
       "        {\n",
       "          raw : DoubleTensor - size: 50\n",
       "          type : tensor\n",
       "          source : \n",
       "            {\n",
       "              node : \n",
       "                {\n",
       "                  forwardFn : table: 0x0ad88978\n",
       "                  outputs : table: 0x0cd55770\n",
       "                  gradientFn : table: 0x0c824478\n",
       "                  inputs : table: 0x0cd55500\n",
       "                  outputTargets : table: 0x0cd55798\n",
       "                }\n",
       "              index : 1\n",
       "              type : computed\n",
       "            }\n",
       "        }\n",
       "      2 : \n",
       "        {\n",
       "          raw : DoubleTensor - size: 10\n",
       "          type : tensor\n",
       "          source : \n",
       "            {\n",
       "              node : \n",
       "                {\n",
       "                  forwardFn : table: 0x0ad88978\n",
       "                  outputs : table: 0x0cd4f268\n",
       "                  gradientFn : table: 0x0c824478\n",
       "                  inputs : table: 0x0cd4eff8\n",
       "                  outputTargets : table: 0x0cd4f290\n",
       "                }\n",
       "              index : 1\n",
       "              type : computed\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "  W : \n",
       "    {\n",
       "      1 : \n",
       "        {\n",
       "          raw : DoubleTensor - size: 100x50\n",
       "          type : tensor\n",
       "          source : \n",
       "            {\n",
       "              node : \n",
       "                {\n",
       "                  forwardFn : table: 0x0c7fd430\n",
       "                  outputs : table: 0x0cd579f8\n",
       "                  gradientFn : table: 0x0c81dce8\n",
       "                  inputs : table: 0x0cd56f30\n",
       "                  outputTargets : table: 0x0cd57a20\n",
       "                }\n",
       "              index : 1\n",
       "              type : computed\n",
       "            }\n",
       "        }\n",
       "      2 : \n",
       "        {\n",
       "          raw : DoubleTensor - size: 50x10\n",
       "          type : tensor\n",
       "          source : \n",
       "            {\n",
       "              node : \n",
       "                {\n",
       "                  forwardFn : table: 0x0c7fd430\n",
       "                  outputs : table: 0x0cd51228\n",
       "                  gradientFn : table: 0x0c81dce8\n",
       "                  inputs : table: 0x0cd50f80\n",
       "                  outputTargets : table: 0x0cd51250\n",
       "                }\n",
       "              index : 1\n",
       "              type : computed\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "}\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 8.7231e-09\n",
       " 3.8310e-03\n",
       "-1.5476e-01\n",
       " 1.1676e-07\n",
       " 4.9298e-06\n",
       " 5.2273e-04\n",
       " 6.4689e-06\n",
       " 2.6721e-02\n",
       " 1.5375e-08\n",
       " 1.0435e-02\n",
       "[torch.DoubleTensor of size 10]\n",
       "\n",
       "\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-- in this case:\n",
    "--> loss: is a scalar (Lua number)\n",
    "--> dparams: is a table that mimics the structure of params; for\n",
    "--  each Tensor in params, dparams provides the derivatives of the\n",
    "--  loss wrt to that Tensor.\n",
    "print(dparams[\"b\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "[string \"for i,sample in datasetIterator() do...\"]:1: attempt to call global 'datasetIterator' (a nil value)\nstack traceback:\n\t[string \"for i,sample in datasetIterator() do...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:177: in function </Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/yutaro/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:344: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/yutaro/.ipython/profile_default/...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "[string \"for i,sample in datasetIterator() do...\"]:1: attempt to call global 'datasetIterator' (a nil value)\nstack traceback:\n\t[string \"for i,sample in datasetIterator() do...\"]:1: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:177: in function </Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/yutaro/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:344: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/yutaro/.ipython/profile_default/...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "for i,sample in datasetIterator() do\n",
    "   -- estimate gradients wrt params:\n",
    "   local grads, loss = dneuralNet(params, sample.x, sample.y)\n",
    "\n",
    "   -- SGD step:\n",
    "   for i = 1,#params.W do\n",
    "      -- update params with an arbitrary learning rate:\n",
    "      params.W[i]:add(-.01, grads.W[i])\n",
    "      params.b[i]:add(-.01, grads.b[i])\n",
    "   end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local d = require 'autograd'\n",
    "d.optimize(true)\n",
    "local innerFn = function(x)\n",
    "   return x^2\n",
    "end\n",
    "local ddf = d(function(params)\n",
    "   local grads = d(innerFn)(params.W + torch.cmul(params.r,params.v))\n",
    "   return torch.sum(grads)\n",
    "end)\n",
    "\n",
    "local params = {\n",
    "    W = torch.Tensor({3,3,3}),\n",
    "    r = torch.Tensor({2,2,2}),\n",
    "    v = torch.Tensor({5,5,5})\n",
    "}\n",
    "gradGrads = ddf(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.DoubleTensor of size 3]\n",
       "\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradGrads.v.raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "local d = require 'autograd'\n",
    "d.optimize(true)\n",
    "local innerFn = function(x)\n",
    "   return x^2\n",
    "end\n",
    "local ddf = d(function(params)\n",
    "   local grads = d(innerFn)(params.W + params.r * params.v)\n",
    "   return grads\n",
    "end)\n",
    "\n",
    "local params = {\n",
    "    W = 5,\n",
    "    r = 3,\n",
    "    v = 2\n",
    "}\n",
    "gradGrads = ddf(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4\t\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(gradGrads.r.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "size mismatch, m1: [1 x 100], m2: [20 x 15] at /tmp/luarocks_torch-scm-1-6334/torch7/lib/TH/generic/THTensorMath.c:770\nstack traceback:\n\t[C]: at 0x0c1780a0\n\t[C]: in function 'fn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:40: in function '__mul'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Value.lua:182: in function 'fn'\n\t.../install/share/lua/5.1/autograd/runtime/codegen/Node.lua:72: in function 'evaluateForward'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:25: in function '__mul'\n\t[string \"local d = require 'autograd'...\"]:57: in function 'fn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:353: in function 'protectedFn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:383: in function 'record'\n\t.../install/share/lua/5.1/autograd/runtime/codegen/init.lua:20: in function <.../install/share/lua/5.1/autograd/runtime/codegen/init.lua:19>\n\t[string \"local d = require 'autograd'...\"]:78: in function 'fn'\n\t...\n\t[string \"local d = require 'autograd'...\"]:92: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:177: in function </Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/yutaro/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:344: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/yutaro/.ipython/profile_default/...\"]:1: in main chunk",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "size mismatch, m1: [1 x 100], m2: [20 x 15] at /tmp/luarocks_torch-scm-1-6334/torch7/lib/TH/generic/THTensorMath.c:770\nstack traceback:\n\t[C]: at 0x0c1780a0\n\t[C]: in function 'fn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:40: in function '__mul'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Value.lua:182: in function 'fn'\n\t.../install/share/lua/5.1/autograd/runtime/codegen/Node.lua:72: in function 'evaluateForward'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:25: in function '__mul'\n\t[string \"local d = require 'autograd'...\"]:57: in function 'fn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:353: in function 'protectedFn'\n\t...install/share/lua/5.1/autograd/runtime/codegen/Graph.lua:383: in function 'record'\n\t.../install/share/lua/5.1/autograd/runtime/codegen/init.lua:20: in function <.../install/share/lua/5.1/autograd/runtime/codegen/init.lua:19>\n\t[string \"local d = require 'autograd'...\"]:78: in function 'fn'\n\t...\n\t[string \"local d = require 'autograd'...\"]:92: in main chunk\n\t[C]: in function 'xpcall'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:177: in function </Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:143>\n\t/Users/yutaro/torch/install/share/lua/5.1/lzmq/poller.lua:75: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:307: in function 'poll'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:325: in function 'sleep_ex'\n\t...rs/yutaro/torch/install/share/lua/5.1/lzmq/impl/loop.lua:370: in function 'start'\n\t/Users/yutaro/torch/install/share/lua/5.1/itorch/main.lua:344: in main chunk\n\t[C]: in function 'require'\n\t[string \"arg={'/Users/yutaro/.ipython/profile_default/...\"]:1: in main chunk"
     ]
    }
   ],
   "source": [
    "local d = require 'autograd'\n",
    "local t = require 'torch'\n",
    "d.optimize(true)\n",
    "\n",
    "-- params = {\n",
    "--    W = {\n",
    "--       t.randn(20,15),\n",
    "--       t.randn(15,10),\n",
    "--    },\n",
    "--    b = {\n",
    "--       t.randn(15),\n",
    "--       t.randn(10),\n",
    "--    }\n",
    "-- }\n",
    "\n",
    "params = {\n",
    "   W = {\n",
    "      t.randn(20,15),\n",
    "      t.randn(15,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   },\n",
    "    r = {\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    },\n",
    "    v = {\n",
    "           -- has to be the same vector as in Hv\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    },\n",
    "    Wb ={    W = {\n",
    "      t.randn(20,15),\n",
    "      t.randn(15,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   }}\n",
    "}\n",
    "-- define model\n",
    "local innerFn = function(params, x, y)\n",
    "   local h1 = t.tanh(x * params.W[1] + params.b[1])\n",
    "   local h2 = t.tanh(h1 * params.W[2] + params.b[2])\n",
    "   local yHat = h2 - t.log(t.sum(t.exp(h2)))\n",
    "   local loss = - t.sum(t.cmul(yHat, y))\n",
    "   return loss\n",
    "end\n",
    "\n",
    "--print(#params2.r.W)\n",
    "\n",
    "--TO DO : change params2.Wb + t.cmul(params2.r, params2.v) so that it handles the nested params nicely. \n",
    "local outerFn = function(params)\n",
    "    local product = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            product.W[i] = t.cmul(params.r.W[i], params.v.W[i])\n",
    "            product.b[i] = t.cmul(params.r.b[i], params.v.b[i])\n",
    "    end\n",
    "    local addition = clone(params.Wb)\n",
    "    for i = 1, #params.r.W do -- has to change if I want to do a different param configuration\n",
    "            addition.W[i] = t.add(params.W[i], product.W[i])\n",
    "            addition.b[i] = t.add(params.b[i], product.b[i])\n",
    "    end   \n",
    "    local grads, loss = d(innerFn)(addition, x, y)\n",
    "    return loss\n",
    "end\n",
    "\n",
    "local ddf = d(outerFn)\n",
    "--gradGrads = d(outerFn)(params)\n",
    "\n",
    "\n",
    "-- local params2 = {\n",
    "--     W = 5,\n",
    "--     r = 3,\n",
    "--     v = 2\n",
    "-- }\n",
    "\n",
    "gradGrads = ddf(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function clone (t) -- deep-copy a table\n",
    "    if type(t) ~= \"table\" then return t end\n",
    "    local meta = getmetatable(t)\n",
    "    local target = {}\n",
    "    for k, v in pairs(t) do\n",
    "        if type(v) == \"table\" then\n",
    "            target[k] = clone(v)\n",
    "        else\n",
    "            target[k] = v\n",
    "        end\n",
    "    end\n",
    "    setmetatable(target, meta)\n",
    "    return target\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- Just checking if paramsss is a table of parameters \n",
    "-- We get something like this from autograd.functionalize\n",
    "-- {\n",
    "--   1 : DoubleTensor - size: 16x27\n",
    "--   2 : DoubleTensor - size: 16\n",
    "--   3 : DoubleTensor - size: 10x1024\n",
    "--   4 : DoubleTensor - size: 10\n",
    "-- }\n",
    "\n",
    "\n",
    "-- require 'nn'\n",
    "-- local autograd = require 'autograd'\n",
    "-- local model = nn.Sequential()\n",
    "-- model:add(nn.SpatialConvolutionMM(3, 16, 3, 3, 1, 1, 1, 1))\n",
    "-- model:add(nn.Tanh())\n",
    "-- model:add(nn.Reshape(16*8*8))\n",
    "-- model:add(nn.Linear(16*8*8, 10))\n",
    "-- model:add(nn.Tanh())\n",
    "-- -- Note that this model could have been pre-trained, and reloaded from disk.\n",
    "\n",
    "-- -- Functionalize the model:\n",
    "-- local modelf, paramsss = autograd.functionalize(model)\n",
    "\n",
    "-- print(paramsss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 100\n",
       "  50\n",
       "[torch.LongStorage of size 2]\n",
       "\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local params = {\n",
    "   W = {\n",
    "      t.randn(100,50),\n",
    "      t.randn(50,10),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(50),\n",
    "      t.randn(10),\n",
    "   }\n",
    "}\n",
    "\n",
    "print(params.W[1]:size())\n",
    "-- for i = 1,#params.W do\n",
    "--       -- update params with an arbitrary learning rate:\n",
    "--       params.W[i]:add(-.01, grads.W[i])\n",
    "--       params.b[i]:add(-.01, grads.b[i])\n",
    "--    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "-- This function will do element wise addition on two tables of parameters. \n",
    "\n",
    "function addParams(paramsA, paramsB)\n",
    "    for i = 1, 2 do\n",
    "        paramsA.W[i]:add(paramsB.W[i])\n",
    "        paramsA.b[i]:add(paramsB.b[i])\n",
    "    end\n",
    "    return paramsA\n",
    "end\n",
    "\n",
    "-- This function will do element wise multiplication on two tables of parameters.\n",
    "function cmulParams(paramsA, paramsB)\n",
    "    for i = 1, #paramsA.W do\n",
    "        paramsA.W[i]:cmul(paramsB.W[i])\n",
    "        paramsA.b[i]:cmul(paramsB.b[i])\n",
    "    end\n",
    "    return paramsA\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "   W = {\n",
    "      t.Tensor(20,15):fill(1),\n",
    "      t.Tensor(15,10):fill(2),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   }\n",
    "}\n",
    "\n",
    "params2 = {\n",
    "   W = {\n",
    "      t.Tensor(20,15):fill(10),\n",
    "      t.Tensor(15,10):fill(12),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   }\n",
    "}\n",
    "\n",
    "for i = 1, #params.W do\n",
    "    params.W[i]:add(params2.W[i])\n",
    "    params.b[i]:add(params2.b[i])\n",
    "end\n",
    "\n",
    "--ans  = addParams(params1,params2)\n",
    "\n",
    "-- local params2 = {\n",
    "--     Wb = copy(params),\n",
    "--     r = {\n",
    "--             W = {\n",
    "--               t.randn(20,15),\n",
    "--               t.randn(15,10),\n",
    "--                },\n",
    "--             b = {\n",
    "--               t.randn(15),\n",
    "--               t.randn(10),\n",
    "--                }\n",
    "--           },\n",
    "--     v = {\n",
    "--            -- has to be the same vector as in Hv\n",
    "--     }\n",
    "-- }\n",
    "\n",
    "params2 = {\n",
    "    --Wb = copy(params),\n",
    "    r = {\n",
    "            W = {\n",
    "              t.Tensor(20,15):fill(2),\n",
    "              t.Tensor(15,10):fill(3),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "          },\n",
    "    v = {\n",
    "           -- has to be the same vector as in Hv\n",
    "                    W = {\n",
    "              t.Tensor(20,15):fill(3),\n",
    "              t.Tensor(15,10):fill(4),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    }\n",
    "}\n",
    "\n",
    "product = {\n",
    "       W = {\n",
    "      t.Tensor(20,15):fill(0),\n",
    "      t.Tensor(15,10):fill(0),\n",
    "   },\n",
    "   b = {\n",
    "      t.randn(15),\n",
    "      t.randn(10),\n",
    "   }\n",
    "}\n",
    "product = clone(params)\n",
    "for i = 1, #params2.r.W do -- has to change if I want to do a different param configuration\n",
    "    product.W[i] = torch.cmul(params2.r.W[i], params2.v.W[i])\n",
    "    product.b[i] = torch.cmul(params2.r.b[i], params2.v.b[i])\n",
    "    --product:add(torch.cmul(params2.r.b[i], params2.v.b[i]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       " 12  12  12  12  12  12  12  12  12  12\n",
       "[torch.DoubleTensor of size 15x10]\n",
       "\n"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(product.W[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2\t\n"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params2 = {\n",
    "    Wb = clone(params),\n",
    "    r = {\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "          },\n",
    "    v = {\n",
    "           -- has to be the same vector as in Hv\n",
    "            W = {\n",
    "              t.randn(20,15),\n",
    "              t.randn(15,10),\n",
    "               },\n",
    "            b = {\n",
    "              t.randn(15),\n",
    "              t.randn(10),\n",
    "               }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(#params2.r.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "20100"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
